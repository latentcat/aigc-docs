import {Note} from "../../../components/mdx";
import {AuthorCiaochaos} from "../../../components/ArticleAuthors";
export const metadata = {
  title: 'Brightness ControlNet è®­ç»ƒæµç¨‹',
}


# Brightness ControlNet è®­ç»ƒæµç¨‹



<Note>
  å°šæœªå®Œæˆ
</Note>



ä½œè€…ï¼š<AuthorCiaochaos />


## ç®€ä»‹


ControlNet ä½¿ Stable Diffusion æœ‰äº†ä¸€å±‚é¢å¤–çš„æ§åˆ¶ï¼Œå®˜æ–¹çš„å®ç°ä¸­å¯ä»¥ä»æ·±åº¦ã€è¾¹ç¼˜çº¿ã€OpenPose ç­‰å‡ ä¸ªç»´åº¦æ§åˆ¶ç”Ÿæˆçš„å›¾åƒã€‚è¿™æ¬¡æˆ‘ä»¬å¸Œæœ›é€šè¿‡äº®åº¦ï¼ˆbrightness / grayscaleï¼‰æ§åˆ¶ç”Ÿå›¾ï¼Œä»è€Œå®ç°è€ç…§ç‰‡è¿˜åŸå½©è‰²ã€å¯¹ç°æœ‰å›¾åƒé‡æ–°ç€è‰²ç­‰éœ€æ±‚ã€‚


æœ¬æ–‡å°†è®°å½•å’Œä»‹ç»ä½¿ç”¨ HuggingFace Diffusers è®­ç»ƒ Brightness ControlNet çš„è¿‡ç¨‹ã€‚


## æ•°æ®é›†å‡†å¤‡


æ•°æ®æºï¼š
* [LAION-Aesthetics V1](https://github.com/LAION-AI/laion-datasets/blob/main/laion-aesthetic.md)ï¼ˆLAION ç¾å­¦è¯„åˆ†å¤§äº 7 çš„å­é›†ï¼‰
* [COYO-700M](https://huggingface.co/datasets/kakaobrain/coyo-700m)ï¼ˆåŒ…å« aesthetic_score_laion_v2 è¯„åˆ†ï¼‰


ä¸‹è½½æ•°æ®ï¼š

```python
from img2dataset import download
import shutil
import multiprocessing

def main():
    download(
        processes_count=16,
        thread_count=64,
        url_list="laion2B-en-aesthetic",
        resize_mode="center_crop",
        image_size=512,
        output_folder="../laion-en-aesthetic",
        output_format="files",
        input_format="parquet",
        skip_reencode=True,
        save_additional_columns=["similarity","hash","punsafe","pwatermark","aesthetic"],
        url_col="URL",
        caption_col="TEXT",
        distributor="multiprocessing",
    )

if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()
```


æ„å»º HuggingFace Datasetsï¼Œä¿å­˜æœ¬åœ°å¹¶æ¨è‡³ Hubï¼š


```python
import os
from datasets import Dataset
from pathlib import Path
from PIL import Image

data_dir = Path(r"H:\DataScience\laion-en-aesthetic")

def entry_for_id(image_folder, filename):
    img = Image.open(image_folder / filename)
    gray_img = img.convert('L')
    caption_filename = filename.replace('.jpg', '.txt')

    with open(image_folder / caption_filename) as f:
        caption = f.read()
    return {
        "image": img,
        "grayscale_image": gray_img,
        "caption": caption,
    }

max_images = 1000000

def generate_entries():
    index = 0

    # cc3m çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
    image_folders = [f.path for f in os.scandir(data_dir) if f.is_dir()]
    for image_folder in image_folders:

        image_folder = Path(image_folder)
        print(image_folder)

        # cc3m å­æ–‡ä»¶å¤¹çš„æ‰€æœ‰æ–‡ä»¶
        for filename in os.listdir(image_folder):
            if not filename.endswith('.jpg'):
                continue
            yield entry_for_id(image_folder, filename)
            index += 1
            if index >= max_images:
                break

        if index >= max_images:
            break

ds = Dataset.from_generator(generate_entries, cache_dir="./.cache")
ds.save_to_disk("./grayscale_image_aesthetic_1M")
ds.push_to_hub('ioclab/grayscale_image_aesthetic_1M', private=True)
```


## è®­ç»ƒè¿‡ç¨‹


ä½¿ç”¨ [ControlNet training example](https://github.com/huggingface/diffusers/tree/main/examples/controlnet) è„šæœ¬è®­ç»ƒï¼Œå…·ä½“å‚æ•°å¦‚ä¸‹ï¼š

```shell
accelerate launch train_controlnet_local.py \
 --pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5" \
 --output_dir="./output_v1a2u" \
 --dataset_name="./grayscale_image_aesthetic_100k" \
 --resolution=512 \
 --learning_rate=1e-5 \
 --image_column=image \
 --caption_column=caption \
 --conditioning_image_column=grayscale_image \
 --train_batch_size=16 \
 --gradient_accumulation_steps=4 \
 --num_train_epochs=2 \
 --tracker_project_name="control_v1a2u_sd15_brightness" \
 --enable_xformers_memory_efficient_attention \
 --checkpointing_steps=5000 \
 --hub_model_id="ioclab/grayscale_controlnet" \
 --report_to wandb \
 --push_to_hub
```

wandb åå°æ•°æ®ï¼š

A6000 GPU è®­ç»ƒæ—¶é•¿ï¼š13hï¼Œsample_countï¼š100kï¼Œepochï¼š1ï¼Œbatch_sizeï¼š16ï¼Œgradient_accumulation_stepsï¼š1ã€‚

![](/assets/brightness-controlnet/10.jpg)

TPU v4-8 GPU è®­ç»ƒæ—¶é•¿ï¼š25hï¼Œsample_countï¼š3mï¼Œepochï¼š1ï¼Œbatch_sizeï¼š2ï¼Œgradient_accumulation_stepsï¼š25ã€‚

[è®­ç»ƒæŠ¥å‘Š](https://api.wandb.ai/links/ciaochaos/oot5cui2)ï¼š

![](/assets/brightness-controlnet/5.jpg)

![](/assets/brightness-controlnet/8.jpg)


Google æä¾›çš„ TPU v4-8 çš„æœºå™¨ï¼Œé…ç½®äº† 240 æ ¸ 480 çº¿ç¨‹ CPUã€400GB å†…å­˜ã€128GB TPU å†…å­˜ã€2000Mbps å¸¦å®½ã€3TB ç£ç›˜ã€‚

ç²—æµ…è®¡ç®—ï¼ŒTPU v4-8 bf16 è¾ƒå•å— A6000 fp32 æœ‰ 15 å€çš„é€Ÿåº¦æå‡ã€‚

![](/assets/brightness-controlnet/6.jpg)

![](/assets/brightness-controlnet/7.jpg)





ControlNet è®ºæ–‡ä¸­æåˆ°çš„ â€Sudden Convergenceâ€œ ç°è±¡ï¼š

![](/assets/brightness-controlnet/11.jpg)

![](/assets/brightness-controlnet/12.jpg)





## æ•ˆæœ


![](/assets/brightness-controlnet/1.jpg)

![](/assets/brightness-controlnet/2.jpg)

![](/assets/brightness-controlnet/3.jpg)

![](/assets/brightness-controlnet/4.jpg)




## å‚è€ƒèµ„æ–™

* [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)

  ControlNet è®ºæ–‡ã€‚æœ‰åŸç†è§£é‡Šã€è®­ç»ƒå‚æ•°ã€å¯¹æ¯”å›¾ç­‰é‡è¦ä¿¡æ¯ã€‚

* [ControlNet - GitHub](https://github.com/lllyasviel/ControlNet)

  ControlNet å®˜æ–¹ä»“åº“ï¼ŒåŒ…æ‹¬ä¸€ä¸ª[è®­ç»ƒæ•™ç¨‹](https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md)ã€‚

* [ControlNet 1.1 - GitHub](https://github.com/lllyasviel/ControlNet-v1-1-nightly)

  ControlNet 1.1 Nightlyã€‚

* [Train your ControlNet with diffusers ğŸ§¨](https://huggingface.co/blog/train-your-controlnet)

  HuggingFace å®˜æ–¹ä½¿ç”¨ Diffusers è®­ç»ƒ ControlNet çš„æ•™ç¨‹ï¼Œéå¸¸è¯¦å°½ã€‚

* [ControlNet training example](https://github.com/huggingface/diffusers/tree/main/examples/controlnet)

  HuggingFace ControlNet è®­ç»ƒè„šæœ¬æ¡ˆä¾‹ã€‚

* [JAX/Diffusers community sprint ğŸ§¨](https://github.com/huggingface/community-events/tree/main/jax-controlnet-sprint)

  HuggingFace Ã— Google ç¤¾åŒºå†²åˆºæ´»åŠ¨æ–‡æ¡£ã€‚


